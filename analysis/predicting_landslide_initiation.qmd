---
title: "Predicting landslide initiation"
author: Julia Lober
format: html
editor: visual
---

```{r load, include = FALSE}
library(terra)

library(mlr3)
library(mlr3spatial)
library(mlr3spatiotempcv)
library(mlr3pipelines)
library(mlr3learners)
```

## Predicting landslide initiation

We start by predicting on 1 small basin as part of 1 of the watershed designations of DOGAMI in Oregon. Find the data wherever it is located on your computer.

```{r}
dir <- "//Mac/Home/Documents/terrainworks/code/sandbox/data/downloaded_3.6/in/"
dem <- terra::rast(paste0(dir, "Umpqua/basin_1.flt"))
pts <- terra::vect(paste0(dir, "all_initiation_points.shp"))

pts_local <- crop(pts, dem)

plot(dem)
plot(pts_local, 
     add = TRUE)
```

### Calculating the predictors

The predictors chosen by our model are:

1.  Partial contributing area with a hydraulic conductivity of 1 and storm duration of 48 hours.

2.  Mean curvature

3.  Distance to road

4.  Stand age

We need to calculate the partial contributing area and mean curvature using the executables included in this package. There is also a function for calculating the distance to a road, and the stand age can be collected from available data.

The model is trained on all of the available inventory points and stored in "logreg_learner". This happens in the comparing_landslide_models.qmd file.

```{r}
scratch <- "//Mac/Home/Documents/terrainworks/code/sandbox/data/scratch/"
scratch <- "/Users/julialober/Documents/terrainworks/code/sandbox/data/scratch/"

rasters <- c(paste0("GRADIENT,", scratch, "gradient"),
             paste0("MEAN CURVATURE,", scratch, "mean_curv"))

length_scale <- 15

rasters <- elev_deriv(rasters = rasters,
                length_scale = length_scale,
                dem = paste0(dir, "Umpqua/basin_1.flt"),
                scratch_dir = scratch)
```

Now, calculate partial contributing area. The 48-hour duration was the best across all of the model investigating, so we only need to calculate that one.

```{r}
rasters <- c(rasters, contributing_area(raster = paste0(scratch, "pca_k1_d48"),
                    dem = paste0(dir, "Umpqua/basin_1.flt"), 
                    length_scale = 15, 
                    k = 1, 
                    d = 48, 
                    scratch_dir = scratch))
```

Stand age data comes from 2017. It has to be first reprojected into the UTM zone 10 CRS and then resampled to the same extent and resolution as the DEM. Then, we can extract the values and match them with the values in the topographical parameters DEM.

```{r}
age_fn <- "//Mac/Home/Documents/terrainworks/code/sandbox/data/downloaded_3.6/in/age_dom_2017.tif"

# \\Mac\Home\Documents\terrainworks\code\sandbox\data\downloaded_3.6\in
# "\\Mac\Home\Documents\terrainworks\code\sandbox\data\downloaded_3.6\in\age_dom_2017.tif"

age_rast <- terra::rast(age_fn)

age_proj <- project(age_rast, "epsg:26910")
age_resampled <- resample(age_proj, dem, 'near')
```

```{r}
plot(age_resampled)
```

Now that the scripts have been run, we can assemble the data from files (and never run the 2 hour script again).

```{r}
scratch <- "/Users/julialober/Documents/terrainworks/code/sandbox/data/scratch/"

topo_files <- c(paste0("GRADIENT,", scratch, "gradient.flt"),
                paste0("MEAN CURVATURE,", scratch, "mean_curv.flt"))

topo_rast <- elev_deriv(rasters = topo_files)
pca_rast <-  contributing_area(raster = paste0(scratch, "pca_k1_d48.flt"))

rasters <- c(topo_rast, pca_rast)
```

### Success rate curves

To evaluate the model using proportions, we use a success rate curve. The curve has the proportion of the total area on the x-axis, with the proportion of landslides on the y-axis. To generate this curve, we train a model on the sample points, then predict the entire DEM. The sampling to generate training points and model decisions have been made in a different document: we use a logistic regression learner trained on the mean curvature, log value of partial contributing area with a duration of 48 hours, and gradient.

```{r}
folder <- "/Users/julialober/Documents/terrainworks/code/sandbox/data/downloaded_3.6/out/"
load(paste0(folder, "ls_1996.Rdata"))
load(paste0(folder, "ls_2007.Rdata"))
load(paste0(folder, "ls_2011.Rdata"))
load(paste0(folder, "nonls_1996.Rdata"))
load(paste0(folder, "nonls_2007.Rdata"))
load(paste0(folder, "nonls_2011.Rdata"))

training_data <- rbind(ls_1996,
                       ls_2007,
                       ls_2011,
                       nonls_1996,
                       nonls_2007,
                       nonls_2011)

training_data <- subset(training_data, select = c("gradient", "mean_curv", "pca_k1_48", "class", "x", "y"))
training_data$pca_k1_48 <- log(training_data$pca_k1_48, base = 2)
training_data$class <- as.factor(training_data$class)
```

```{r}
landslide_task <- as_task_classif_st(x = training_data, 
                                    id = "landslide_initiation", 
                                    target = "class", 
                                    positive = "pos", 
                                    coordinate_names = c("x", "y"), 
                                    crs = "epsg:26910")

pl_feature_eng <- 
  po("scale") %>>% 
  po("encode") 

pl_feature_eng$train(landslide_task)
landslide_task_eng <- pl_feature_eng$predict(landslide_task)$encode.output

logreg_learner <- lrn("classif.log_reg", 
                      predict_type = "prob")

logreg_learner$train(landslide_task_eng)
logreg_learner$model
```

Now, we are ready to predict the basin area. We need to extract the all the data points from the DEM and make sure the names match what is expected by the model (gradient, mean_curv, and pca_k1_48).

```{r}
names(rasters) <- c("gradient", "mean_curv", "pca_k1_48")
predicting_data <- as.data.frame(rasters, xy = TRUE)
```

The mlr3 package support spatial prediction with the predict_spatial function. This function accepts new data in raster format and outputs a raster, but I can't figure out if the function will support a probability output instead of a binary class output.

```{r}
predictions <- predict_spatial(rasters, logreg_learner)
```

For now, I'll try predicting on a data frame. The downside of this strategy is the extra time needed to convert the DEM to a data frame, and the lack of space optimization that the spatial_predict function allows.

```{r}
pd <- predicting_data
pd$pca_k1_48 <- log(predicting_data$pca_k1_48, base = 2)

# manually pre process the data: take the log of the pca and center+scale the data
center_vals <- pl_feature_eng$pipeops$scale$state$center
scale_vals <- pl_feature_eng$pipeops$scale$state$scale

pd$gradient <- (pd$gradient - center_vals[1]) / scale_vals[1]
pd$mean_curv <- (pd$mean_curv - center_vals[2]) / scale_vals[2]
pd$pca_k1_48 <- (pd$pca_k1_48 - center_vals[3]) / scale_vals[3]

predictions_df <- logreg_learner$predict_newdata(pd)
```

```{r}
plot(predictions)

```

```{r}
predictions_dt <- as.data.table(predictions_df)
predictions_dt <- cbind(predictions_dt, predicting_data)


```
