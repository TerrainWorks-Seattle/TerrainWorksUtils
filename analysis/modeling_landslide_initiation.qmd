---
title: "Modeling Landslide Initiation"
autho: Julia Lober
format: html
editor: visual
---

```{r setup}
#| echo: false
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  error = FALSE
)
```

```{r load, include = FALSE}
library(mlr3)
library(mlr3spatial)
library(mlr3spatiotempcv)
library(mlr3learners)
library(mlr3tuning)
library(mlr3viz)

# Packages for model-building
library(glmnet)
library(ranger)

# Packages for other things
library(terra)
library(ggplot2)
library(maps)
```

```{r}
source("../R/create_model_dataframe.R")
sample_pnts <- training_data[!(is.na(training_data$dist_to_road)), 2:length(training_data)]
```

## A look at the data.

The first step is to look at the data we are using to built the model. We are working with data files that have been assembled by the PFA_Sampling_Multiple_DEMs documents included in this package. This package produces .Rdata files that contain data frames of landslide points divided by the approximate year in which they occurred and non-landslide points sampled from the surrounding area divided in the same way.

The data frames contain a variety of predictors selected based on the physical parameters that we know control landslides. These predictors have various sources of data which will be explained later.

```{r}
#| echo: false
head(sample_pnts)
```

```{r}
#| echo: false
points <- vect(cbind(sample_pnts[, "x"], y = sample_pnts[, "y"]))
points$yr <- sample_pnts$year
points$class <- sample_pnts$class
crs(points) <- "epsg:26910"
points <- project(points, "epsg:4326")

load("/Users/julialober/Documents/terrainworks/code/sandbox/data/oregon_map.Rdata")

ggplot() + 
  geom_polygon(data = oregon, mapping = aes(x = long, y = lat),
               color = "white", fill = "white") +
  # geom_spatvector(data = points[points$class == "neg", ],
  #                 mapping = aes(shape = as.factor(points[points$class == "neg", ]$yr)),
  #                 show.legend = TRUE, size = 1 , color = "black", alpha = 1) +
  geom_spatvector(data = points[points$class == "pos", ],
                  mapping = aes(shape = as.factor(points[points$class == "pos", ]$yr)),
                  show.legend = TRUE, size = 1, color = "red", alpha = 0.4) +
  scale_shape_manual(values = c(3, 1, 17), labels = c("1996 (213)", "2007 (137)", "2011 (47)")) + 
  labs(x = "Longitude", y = "Latitude", shape = "Study points (count)")
```

Note that the landslide initiation points tend to come in clusters and the years 1996 and 2007 have much many more points located than 2011. Negative landslide points are randomly selected within a buffer area near the points. There are other documents that explain the steps and decisions to assemble this data set, so I will not go into it any further.

## Defining the modeling task and learner

For `mlr3`, we need to define the task, the type of model (called a learner), and the resampling method that we want to use. The task is our landslide initiation data set and the learner (for now) is a logistic regression model. If we want to add a different type of model, this will be done in a different document.

There are several steps we need to work through:

1.  Feature selection - Many options exist for feature selection methods that vary slightly in the way that they attribute value to different predictors and iterate through subsets. We explore a small selection in the feature_selection.qmd document, select a basic method from there.
2.  Tuning hyperparameters - This step is only necessary if we decide to use a model that has hyperparameters, like a random forest model. Otherwise, we will ignore it.
3.  Estimating performance - Here, we use a spatial cross-validation resampling method to assess the model performance while minimizing bias. By using spatial cross-validation, we keep testing and training sets closer to independent inputs, which should give us a more realistic guess of how accurate the model will on new data.
4.  Generating a proportion raster - Once we have the best model option as assessed from the previous steps, we can input all of the DEMs to produce a probability raster, which we will then convert to a proportion raster.

```{r}
set.seed(12345)
landslide_task <- as_task_classif_st(x = sample_pnts, 
                                    id = "landslide_initiation", 
                                    target = "class", 
                                    positive = "pos", 
                                    coordinate_names = c("x", "y"), 
                                    crs = "epsg:26910")

logreg_learner <- lrn("classif.log_reg", 
                      predict_type = "prob")

rf_learner <- lrn("classif.ranger", 
                  predict_type = "prob")



landslide_resamp <- rsmp("repeated_spcv_coords", 
                         folds = 5, 
                         repeats = 3)
```

These are the first four folds of the clustering spatial resampling.

```{r}
#| echo: false
autoplot(landslide_resamp, 
         landslide_task, 
         fold_id = c(1:4))
```

### 1. Feature selection

The rule in machine learning is to create the best model possible with the fewest features needed. This not only makes the model quicker to train, but also makes it easier to predict (smaller data inputs) and also reduces the risk of over-fitting.

```{r}

```

## Training the logistic regression model

To train the logistic regression model, which has no hyperparameters that need to be tuned, we call the method resample.

```{r}
build_time <- system.time(logreg_model <- resample(task= landslide_task, 
                                            learner = logreg_learner, 
                                            resampling = landslide_resamp))

build_time
```

And now we look at the results.

```{r}
logreg_model
```

## Training the random forest model

```{r}
build_time <- system.time(rf_model <- resample(task= landslide_task, 
                                            learner = rf_learner, 
                                            resampling = landslide_resamp))
```

## Compare the two models

```{r}
autoplot(c(logreg_model, rf_model), measure = msr("classif.auc"))
```
