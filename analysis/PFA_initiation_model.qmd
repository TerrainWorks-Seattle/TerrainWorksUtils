---
title: "PFA Landslide Initiation Model"
author: "Julia Lober"
format: 
  html:
    toc: true
    number-section: true
    highlight-style: github
date: May 1, 2023
editor: visual
params: 
  data_dir: 
    label: Data Directory
    value: /Users/julialober/Documents/terrainworks/code/sandbox/data/downloaded_3.6/out/
    input: text
  pred_dir: 
    label: Data Directory
    value: /Users/julialober/Documents/terrainworks/code/sandbox/data/predicting_input/
    input: text
  init_pts: 
    label: Data Directory
    value: /Users/julialober/Documents/terrainworks/code/sandbox/data/downloaded_3.6/in/all_initiation_points.shp
    input: text
---

```{r setup}
#| echo: false
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  error = FALSE
)
```

```{r load, include = FALSE}
# Modeling packages
library(mlr3)
library(mlr3spatial)
library(mlr3spatiotempcv)
library(mlr3learners)
library(mlr3tuning)
library(mlr3viz)
library(mlr3fselect)
library(mlr3filters)
library(mlr3pipelines)

# Packages for model-building
library(glmnet)
library(ranger)

# Packages for other things
library(terra)
library(ggplot2)
library(viridis)
library(patchwork)
library(ggpubr)
library(maps)
library(tidyterra)
library(data.table)
library(tictoc)

library(TerrainWorksUtils)
```

# Landslide initiation susceptibility model for the Private Forest Accord

This document outlines the process for making the landslide initiation model for the Oregon Private Forest Accord project. We work with data generated from the PFA_Sampling_Multiple_DEMs document that is included in this package. The primary focus of this document is to show the model-building process and decisions that resulted in the final model used in combination with the landslide runout model to generate riparian buffers for the Private Forest Accord. Broadly speaking, there were 3 major decisions made in this process:

1.  A method for feature selection.

2.  The type of model built.

3.  A spatio-temporal resampling strategy.

## The data

Locate the data files and assemble into one data frame for model training. We use the natural log the partial contributing area values, as it shows a much more symmetrical distribution, which should be a better input for the model.

```{r}
# Load all of the data
list_files <- c("ls_1996.Rdata", "ls_2007.Rdata", "ls_2011.Rdata", 
                "nonls_1996.Rdata", "nonls_2007.Rdata", "nonls_2011.Rdata")
capture.output(lapply(paste0(params$data_dir, list_files), load, .GlobalEnv), file = 'NUL')
ls_2011$geo <- ls_2011$geo$GeoClass
ls_2007$geo <- ls_2007$geo$GeoClass

# Concatenate into one data frame
data <- rbind(transform(ls_1996, year = "1996"),
              transform(ls_2007, year = "2007"),
              transform(ls_2011, year = "2011"),
              transform(nonls_1996, year = "1996"),
              transform(nonls_2007, year = "2007"),
              transform(nonls_2011, year = "2011"))

sample_pnts <- data[!(is.na(data$dist_to_road)), 2:length(data)]
sample_pnts$geo <- as.factor(sample_pnts$geo)
sample_pnts$class <- as.factor(sample_pnts$class)

# Take the natural log of the PCA values for a more uniform distribution
sample_pnts$ln_pca_k1_48 <- log(sample_pnts$pca_k1_48)
sample_pnts$ln_pca_k1_24 <- log(sample_pnts$pca_k1_24)
sample_pnts$ln_pca_k1_12 <- log(sample_pnts$pca_k1_12)
sample_pnts$ln_pca_k1_6 <- log(sample_pnts$pca_k1_6)

# Add a few transformed parameters to look at 
sample_pnts$gradient_sq <- sample_pnts$gradient ^ 2
sample_pnts$grad_by_pca <- sample_pnts$gradient * sample_pnts$ln_pca_k1_48

```

```{r}
#| echo: false
points <- vect(cbind(sample_pnts[, "x"], y = sample_pnts[, "y"]))
points$yr <- sample_pnts$year
points$class <- sample_pnts$class
crs(points) <- "epsg:26910"
points <- project(points, "epsg:4326")

load("/Users/julialober/Documents/terrainworks/code/sandbox/data/oregon_map.Rdata")

ggplot() + 
  geom_polygon(data = oregon, mapping = aes(x = long, y = lat),
               color = "white", fill = "white") +
  geom_spatvector(data = points[points$class == "pos", ],
                  mapping = aes(color = as.factor(points[points$class == "pos", ]$yr), shape = as.factor(points[points$class == "pos", ]$yr)),
                  show.legend = TRUE, size = 1, alpha = 0.6) +
  scale_color_manual(name = "Landslide points (count)", values = viridis(4)[1:3], labels = c("1996 (213)", "2007 (137)", "2011 (47)")) + 
  scale_shape_manual(name = "Landslide points (count)",values = c(1, 4, 17), labels = c("1996 (213)", "2007 (137)", "2011 (47)")) + 
  labs(x = "Longitude", y = "Latitude")
```

The data includes 397 landslide initiation points from the inventory assembled by the Oregon Department of Geology and Mineral Resources, and combine data points from a couple different storm studies. There are 3 broad temporal groupings roughly corresponding to the years 1996, 2007, and 2011. The majority of the points come from the 1996 storm study.

The reader should refer for PFA_Sampling_Multiple_DEMs for details on how the negative points are selected. We use an imbalanced sample with 10x the number of non-landslide points as landslide points. The benefit of using more non-landslide points is a more accurate representation of the non-landslide area. An imbalanced sample for our linear regression model is likely to change the magnitude of the probabilities outputted but shouldn't change the marginal probabilities. Since much of our model analysis will be a comparison of proportions, where probabilities are integrated over total probabilities and area to produce a value for number of landslides, a uniform change in probabilities shouldn't impact our observations too much.

### Covariates

Candidate covariates were chosen based on our understanding of the physical properties that control landslide initiation. These include physical elevation derivatives calculated based on lidar DEMs, partial contributing area for a given storm duration, total contributing area, stand age, distance to a road, and geological sediment type. Although we look at models that include stand age and distance to a road, these covariates should actually be left out of the final model as they will change over time.

The elevation derivatives that we consider are mean curvature and gradient (or slope angle). These were selected on the basis of the stability model which includes those as inputs for determining when a slope will fail. Steeper slopes are more likely to slide, as are more convergent slopes(?). Partial contributing area is calculated based on a hydraulic conductivity for the soil (which we assign a value of 1) and the storm duration. We looked at a variety of storm durations, since that reflects the conditions that we are actually interested in.

It is helpful to look at the how the distributions of predictors vary between our landslide points and our non-landslide points. This informs our expectations for which features will be the most important for the model and increases our confidence in the results of feature selection processes.

```{r}
#| echo: false

p1 <- ggplot(data = sample_pnts) + 
  geom_density(aes(x = gradient, fill = class), alpha = 0.4, color = NA) + 
  labs(x = "Gradient", y = "") + 
  # scale_color_manual(values = c("black", "red"), labels = c("Non-landslide points", "Landslide points"), name = "") + 
  scale_fill_manual(values = c("black", "red"), labels = c("Non-landslide points", "Landslide points"), name = "")

p2 <- ggplot(data = sample_pnts) + 
  geom_density(aes(x = mean_curv, fill = class), alpha = 0.4, color = NA) +
  labs(x = "Mean curvature", y = "") + 
  scale_fill_manual(values = c("black", "red"), labels = c("Non-landslide points", "Landslide points"), name = "")

p3 <- ggplot(data = sample_pnts) + 
  geom_density(aes(x = ln_pca_k1_48, fill = class), alpha = 0.4, color = NA) +
  labs(x = "ln(PCA), 48 hr duration", y = "") + 
  scale_fill_manual(values = c("black", "red"), labels = c("Non-landslide points", "Landslide points"), name = "")

p4 <- ggplot(data = sample_pnts) + 
  geom_density(aes(x = age, fill = class), alpha = 0.4, color = NA) +
  labs(x = "Stand age", y = "") + 
  scale_fill_manual(values = c("black", "red"), labels = c("Non-landslide points", "Landslide points"), name = "") + 
  xlim(0, 300)

p5 <- ggplot(data = sample_pnts) + 
  geom_density(aes(x = dist_to_road, fill = class), alpha = 0.4, color = NA) +
  labs(x = "Distance to a road", y = "") + 
  scale_fill_manual(values = c("black", "red"), labels = c("Non-landslide points", "Landslide points"), name = "") + 
  xlim(0, 600)

p6 <- ggplot(data = sample_pnts) + 
  geom_density(aes(x = grad_by_pca, fill = class), alpha = 0.4, color = NA) +
  labs(x = "Gradient * ln(PCA), 48 hr duration", y = "") + 
  scale_fill_manual(values = c("black", "red"), labels = c("Non-landslide points", "Landslide points"), name = "")

p7 <- ggplot(data = sample_pnts) +
  geom_density(aes(x = ln_pca_k1_6, fill = class), alpha = 0.4, color = NA) +
  labs(x = "ln(PCA), 6 hr duration", y = "") + 
  scale_fill_manual(values = c("black", "red"), labels = c("Non-landslide points", "Landslide points"), name = "")


p8 <- ggplot(data = sample_pnts) +
  geom_density(aes(x = total_accum, fill = class), alpha = 0.4, color = NA) +
  labs(x = "Total accumulation area", y = "") + 
  scale_fill_manual(values = c("black", "red"), labels = c("Non-landslide points", "Landslide points"), name = "") +
  xlim(0, 150) 

# ggarrange(p1, p2, p7, p3, common.legend = TRUE)
combo1 <- p1 + p2 + p7 + p3 +
  plot_layout(guides = "collect") & 
  theme(legend.position = "top")
combo1
```

```{r}
#| echo: false
combo2 <- p4 + p5 + p6 + p8 +
  plot_layout(guides = "collect") & 
  theme(legend.position = "top")
combo2
```

These plots show us the indicate the features where the biggest differences are noticed between landslide points and non-landslide points. Notably, the distributions of gradient between landslide and non-landslide points do not vary as much as some of the other covariate candidates. This is surprising, given our physical understanding of slope angle as a major factor in landslide initiation. It is important to note that the sampled non-landslide points are limited to the range of gradient values observed in the landslide inventory. That is, any point with too low or too high of a slope angle is assumed to have zero probability of landslide initiation and is left out of this study. This filtering method is called an analysis mask and the details on what this looks like can be found in the PCA_Sampling_Multiple_DEMs document.

Features with noticeably different distributions between landslide and non-landslide points include mean curvature and partial contributing area.

## Logistic Regression model

A logistic regression model was primarily because it is simple to code into FORTRAN, where the bulk of the data will be run through, and because it performed just as well as a Random Forest model during preliminary comparisons.

A logistic regression model is a type of linear classifier. The classifier finds coefficients for the covariates that minimizes some error value or maximizes accuracy (this measure can be specified). We are not interested in a binary classification for the issue of landslide initiation, so we look at the probability that a given point is classified in the positive class; in other words, the probability landslide initiation at a location.

We are using the `mlr3` ecosystem for modeling in R. This package is generally considered the newest development in machine learning modeling in R - it is more nimble than some other modeling packages and is being updated and expanded frequently. It operates using an object-oriented design. `Tasks` are used to establish what data you are training and testing with, as well as what the prediction goal is. We specify our task as a spatio-temporal task, which allows us to use a spatial resampling method.

```{r}
training_cols <- c("x", "y", "gradient", "mean_curv", "dist_to_road", "total_accum", "geo", "age", "class", "ln_pca_k1_48", "ln_pca_k1_24", "ln_pca_k1_12", "ln_pca_k1_6", "gradient_sq", "grad_by_pca")

training_cols_less <- !(training_cols %in% c("age", "dist_to_road"))

ls_task <- as_task_classif_st(
  x = sample_pnts[, training_cols], 
  id = "landslide_initiation",
  target = "class", 
  positive = "pos", 
  coordinate_names = c("x", "y"), 
  crs = "epsg:26910"
)

ls_task_less <- as_task_classif_st(
  x = sample_pnts[, training_cols_less], 
  id = "landslide_initiation",
  target = "class", 
  positive = "pos", 
  coordinate_names = c("x", "y"), 
  crs = "epsg:26910"
)
```

## Spatial resampling

A common problem among modeling problems with a spatio-temporal aspect is that traditional resampling methods can lead to overfitting of models and overoptimistic estimations of how the model will perform when shown new data. With traditional resampling methods for training and test set splits, both sets are likely to have representation across the spatio-temporal spread of the data. Due to the nature of spatio-temporal data, new data is often outside the domain of the training data (either temporally or spatially), which means that we are training a model in one domain and then using it on data in a slightly different domain. Spatio-temporal resampling methods prevent this by assigning training and test sets based on spatial or temporal groupings, testing the model on a section of data that it hasn't seen before. This helps to prevent models from being overfit to the specific spatial or temporal patterns that we see in the data.

There are many strategies for spatio-temporal resampling strategies, including k-means clustering, block cross-validation, or temporal validation strategies. The K-means clustering strategy is a good choice because it creates relatively even clusters even with uneven distributions of points, as we see in our data set.

```{r}
set.seed(12345)
rsmp_nonsp <- rsmp("cv", 
                  folds = 5)

autoplot(rsmp_nonsp, ls_task, fold_id = c(1:5), pch = 20, alpha = 0.5) + 
  plot_layout(widths = 1)
```

```{r}
set.seed(12345)
rsmp_clus <- rsmp("spcv_coords", 
                  folds = 5)
autoplot(rsmp_clus, ls_task, fold_id = c(1:5), pch = 20, alpha = 0.5) + 
  plot_layout(widths = 1)
```

Here we see the difference between non-spatial resampling and spatial resampling.

Divisions based on year, or a combination of lithology and year were also considered, but were deemed impractical because of the imbalanced splits that they produced.

## Feature Selection

### Step-forward feature selection

Choosing good features is an essential part of the machine learning process. A good model should use features that are correlated to the output class and independent from each other. The other main goal of feature selection is to choose the smallest possible set of features that produces a good model, which both reduces the runtime of training models and the risk of over-fitting. The definition of a "good model" can be difficult to define, but for our purposes, we use the area under the ROC curve (AUC) as a measure of model performance. A higher AUC indicates a better model, so the feature selection processes will try to maximize this value.

Ideally, we could do an exhaustive search of all possible combinations of features, which leads to 2\^n possible combinations of features. With our 13 features, this is already 8192 possibly configurations. When you add a layer of cross validation resampling (we use 5 folds and 2 repetitions), the runtime of this task rapidly grows, we approach the 10,000s to 100,000s of models to evaluate.

Using the AUC allows us to work in the `mlr3` ecosystem, but ultimately, we want to evaluate the model based on the proportions and generate a success-rate curve. So, we use a combination of filtering methods, feature selection, and success-rate curves to determine which features produce the best model.

Instead, we do a forward stepping feature selection process, called `sequential` in `mlr3fselect`. First, a model is built with a single candidate predictor. The predictor from the single-variable model that performs the best is then chosen, and a two-variable model is trained with that variable and the remaining variable. These steps are repeated until there are no more variables left to add. The model with the highest measure is then chosen (here, AUC). Generally, the best model will turn out to be one of the models that does not include all of the predictors. We create an instance of the feature selector and define our model-type (`learner`), resampling method, evaluation measure, and give it the data (`task`).

```{r}
run <- FALSE

if (run) {
  instance <- fselect(
    fselector = fs("sequential"),
    task =  ls_task,
    learner = lrn("classif.log_reg", predict_type = "prob"),
    resampling = rsmp("repeated_spcv_coords",
                         folds = 5,
                         repeats = 3),
    measure = msr("classif.auc")
  )
}
```

For our task, the best model use age, distance to a road, partial contributing area (48 hr), and mean curvature.

Let's do the same thing, but leave out stand age and distance to road. We are interested in how these variables affect the landslide initiation probability, but don't want to include them in the final model that is linked with the runout model.

```{r}
run <- FALSE

if (run) {
  instance <- fselect(
    fselector = fs("sequential"),
    task =  ls_task_less,
    learner = lrn("classif.log_reg", predict_type = "prob"),
    resampling = rsmp("repeated_spcv_coords",
                         folds = 5,
                         repeats = 3),
    measure = msr("classif.auc")
  )
}
```

### Feature filtering

Another way of evaluating feature importance is through a filter. Filters use a correlation measure (of which there are many to choose from) to evaluate which features are most tied to the output class. A major difference between filters and other feature selection algorithms is that these methods do evaluate which features perform best in a model; they only look at the correlation between two variables. Filters are often used when the number of features is large enough that a feature selection algorithm is impractical to run in a reasonable amount of time. Here, we use it to gather more information about which features may be the most useful.

```{r}
info_filter <- flt("information_gain")
info_filter$calculate(ls_task)
res_info <- as.data.table(info_filter)
res_info$rank <- 1:length(res_info$feature)

cmim_filter <- flt("cmim")
cmim_filter$calculate(ls_task)
res_cmim <- as.data.table(cmim_filter)
res_cmim$rank <- 1:length(res_cmim$feature)

jmi_filter <- flt("jmi")
jmi_filter$calculate(ls_task)
res_jmi <- as.data.table(jmi_filter)
res_jmi$rank <- 1:length(res_jmi$feature)

relief_filter <- flt("relief")
relief_filter$calculate(ls_task)
res_relief <- as.data.table(relief_filter)
res_relief$rank <- 1:length(res_relief$feature)
```

The output of these filters has been scaled and combined in the figure below. Each different method showed relatively similar ordering of the variables. Of note is that the longer durations for the partial contributing areas are consistently given better scores than shorter durations.

```{r}
#| echo: false
filter_res <- res_info
filter_res$InfoGain <- (res_info$score / max(res_info$score))
filter_res$CondMIM <- (res_cmim$score / max(res_cmim$score))
filter_res$JMI <- (res_jmi$score / max(res_jmi$score))
filter_res$Relief <- (res_relief$score / max(res_relief$score))

filter_res.m <- melt(filter_res[, c("feature", "InfoGain", "CondMIM", "JMI", "Relief")])

ggplot(filter_res.m) + 
  geom_bar(aes(x = reorder(feature, -value), y = value, fill = variable), stat = "identity", position = "stack") + 
  scale_fill_manual(values = viridis(5)[1:4], labels = c("Information Gain", "Minimal Conditional \nMutual Information \nMaximization", "Joint Mutual Information", "Relief"), name = "Filter name") + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1, face = "bold"),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank()) +
  labs(x = "", y = "Score") + 
  scale_x_discrete(labels = c("ln(pca), 48 hrs", "mean curvature", "ln(pca), 24 hrs", "total accumulation", "gradient * pca, 48 hrs", "ln(pca), 12 hrs", "ln(pca), 6 hrs", "gradient^2", "gradient", "distance to a road", "geology", "stand age")) + 
  facet_grid(variable ~ .) + 
  theme(legend.position = "none")
  
```

## Training the model

From the feature selection processes, we choose a handful of possible feature selection algorithms and do a more in-depth analysis using success-rate curve as the metric for model performance. This involves training a model and then using the model to predict on DEMs.

We start with a model that uses gradient, 48-hour partial contributing area, and the product of the two.

```{r}
set.seed(12345)
ls_task$select(c("ln_pca_k1_48", "grad_by_pca", "gradient"))

# create a pipeline for data preprocessing
scale_features <- 
  po("scale") %>>% 
  po("encode") 
scale_features$train(ls_task)
ls_task_eng <- scale_features$predict(ls_task)$encode.output

# define the type of model
ls_model <- lrn("classif.log_reg", 
             predict_type = "prob")

model <- mlr3::resample(
  task = ls_task_eng,
  learner = ls_model,
  resampling = rsmp("repeated_spcv_coords",
                    folds = 5,
                    repeats = 50)
)

model$aggregate(msr("classif.auc"))
```

First, we run a whole bunch of spatial resampling iterations to get a confident estimate of how well the model will perform. For this estimation, we are only able to look at measures that can be tested in the framework of `mlr3`. We look at area under the ROC curve (AUC).

Our final model is trained on all of the data, to maximize the use of our landslide inventory.

```{r}
ls_model$train(ls_task_eng)
ls_model$model
```

## Predicting

### Single basin

The predicting directory must have the elevation derivative files for the required predictors.

```{r}
pred_dir <- params$pred_dir

# find and load the data files
topo_files <- c(paste0("GRADIENT,", pred_dir, "Umpqua/basin_1/gradient.flt"))
topo_rast <- elev_deriv(rasters = topo_files)
pca_rast <-  contributing_area(raster = paste0(pred_dir, "Umpqua/basin_1/pca_6_1.flt"))
rasters <- c(topo_rast, pca_rast)
names(rasters) <- c("gradient", "pca_k1_48")

# convert to a data frame and add the feature interactions. 
pd <- as.data.frame(rasters, xy = TRUE)
pd$grad_by_pca <- pd$gradient * pd$pca_k1_48
pd$ln_pca_k1_48 <- log(pd$pca_k1_48)

# manually pre process the data: take the log of the pca and center+scale the data
center_vals <- scale_features$pipeops$scale$state$center
scale_vals <- scale_features$pipeops$scale$state$scale

pd$grad_by_pca <- (pd$grad_by_pca - center_vals[1]) / scale_vals[1]
pd$gradient <- (pd$gradient - center_vals[2]) / scale_vals[2]
pd$ln_pca_k1_48 <- (pd$ln_pca_k1_48 - center_vals[3]) / scale_vals[3]

pd <- subset(pd, select = c("x", "y", "grad_by_pca", "gradient", "ln_pca_k1_48"))
```

```{r}
# predict the basin using scaled and centered data.
predictions_df <- as.data.table(ls_model$predict_newdata(pd))
predictions_df <- cbind(pd, predictions_df)
```

Here we predict on a sub-basin of the larger Umpqua watershed.

```{r}
pts <- terra::vect(params$init_pts)
pts_local <- crop(pts, rasters)

ggplot(predictions_df) + 
  geom_tile(aes(x = x, y = y, fill = prob.pos)) + 
  scale_fill_viridis(option = "A", direction = -1) + 
  geom_spatvector(data = pts_local)
```

### Multiple basin

To generate a success rate curve, we combine multiple basins. The success rate curve is the proportions of landslide predicted over the proportion of the predicted area. A smaller area under the success rate curve indicates that the model was more specific with the areas that it deemed to have a high probability of landslide initiation.

```{r}
basin_list <- list.files(dem_dir, "^basin_[0-9]{1,3}\\.flt", recursive = TRUE)
```

```{r}
model <- ls_model
bins <- as.data.table(seq(0, 1, length.out = 500))
names(bins) <- c("probability") 

count <- 1
cumul_prop <- data.frame("area" = rep(0, 500), 
                         "prob" = rep(0, 500))
pts_prop <- data.frame("x" = 0, 
                       "y" = 0,
                       "prob.pos" = 0, 
                       "prob.cumsum" = 0, 
                       "area.cumsum" = 0, 
                       "ls.prop" = 0)

for (basin in basin_list) { 
  
  basin <- sub("\\..*","", basin)
  
  tic(msg = paste0("loop for ", basin))

  # find and load the data files
  topo_files <- c(paste0("GRADIENT,", pred_dir, basin, "/gradient.flt"))
  topo_rast <- elev_deriv(rasters = topo_files)
  
  pca_file <- list.files(paste0(pred_dir, basin), "^pca_6_[0-9]{1,3}\\.flt", full.names = TRUE)
  print(pca_file)
  pca_rast <-  contributing_area(raster = pca_file)
  count <- count + 1
  rasters <- c(topo_rast, pca_rast)
  names(rasters) <- c("gradient", "pca_k1_48")
  
  predicting_data <- as.data.frame(rasters, xy = TRUE)
  predicting_data$ln_pca_k1_48 <- log(predicting_data$pca_k1_48)
  predicting_data$grad_by_pca <- predicting_data$gradient * predicting_data$pca_k1_48
  
  pd <- predicting_data
  pd <- subset(pd, select = c("x", "y", "grad_by_pca", "gradient", "ln_pca_k1_48"))

  # manually pre process the data: take the log of the pca and center+scale the data
  center_vals <- scale_features$pipeops$scale$state$center
  scale_vals <- scale_features$pipeops$scale$state$scale
  
  pd$grad_by_pca <- (pd$grad_by_pca - center_vals[1]) / scale_vals[1]
  pd$gradient <- (pd$gradient - center_vals[2]) / scale_vals[2]
  pd$ln_pca_k1_48 <- (pd$ln_pca_k1_48 - center_vals[3]) / scale_vals[3]
  
  # predict the basin using scaled and centered data.
  predictions_df <- as.data.table(model$predict_newdata(pd))
  predictions_df <- cbind(predictions_df, pd)
  
  proportions <- as.data.table(subset(predictions_df, select = c("x", "y", "prob.pos")))
  setorder(proportions, cols = "prob.pos") 
  
  # calculate the cumulative sums of area and probabilities
  proportions$prob.cumsum <- cumsum(proportions$prob.pos)
  prob_sum <- max(proportions$prob.cumsum)
  proportions$area.cumsum <- seq(1, length(proportions$x))
  area_sum <- max(proportions$area.cumsum)

  setattr(proportions, "sorted", "prob.pos")
  
  binned_prop <- proportions[J(bins$probability), roll = "nearest"]
  cumul_prop$area <- cumul_prop$area + binned_prop$area.cumsum
  cumul_prop$prob <- cumul_prop$prob + binned_prop$prob.cumsum
  
  toc()
}


total_area <- max(cumul_prop$area)
total_prob <- max(cumul_prop$prob)

cumul_prop$area_prop <- cumul_prop$area / total_area
cumul_prop$prob_prop <- cumul_prop$prob / total_prob

dt <- cbind(bins, cumul_prop)[J(pts_prop$prob.pos), roll = "nearest", on = "probability"]
pts_prop$area_prop <- dt$area_prop
```

```{r}
ggplot(cbind(bins, cumul_prop)) + 
  geom_line(aes(x = area_prop, y = prob_prop), color = "red") + 
  geom_point(aes(x = area_prop, y = prob_prop), color = "red") + 
  scale_x_continuous(limits = c(0, 1), expand = c(0, 0)) +
  scale_y_continuous(limits = c(0, 1), expand = c(0, 0)) +
  labs(x = "Proportion of area", y = "Proportion of total probability") + 
  theme(panel.grid = element_blank(),
        panel.background = element_rect(fill = "white", linewidth = 1, color = "black",), 
        panel.border = element_rect(linewidth = 1, color = NA, fill = NA)) 
```
